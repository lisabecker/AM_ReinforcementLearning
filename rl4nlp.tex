\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[round]{natbib}
%\usepackage[style=authoryear,sorting=ynt]{biblatex}
\bibliographystyle{plainnat}
\usepackage[hyperref]{acl2018}
\usepackage{times}
\usepackage{latexsym}
\usepackage{url}
\usepackage{array}
\usepackage{graphicx}
\usepackage[font=small,labelfont=bf]{caption}
\graphicspath{ {./img/} } 
\pagestyle{plain}

\title{{\LARGE Reinforcement Learning for Natural Language Processing}\\[1.5mm]
{\large Final paper: Literature Review Article}\\[1.5mm]} 
\author{Author: Lisa Becker (775242) } 

\begin{document}
\maketitle
\section{Abstract}


\section{Introduction}
In 2016, DeepMind put Reinforcement Learning (RL) in the spotlights by developing AlphaGo \citep{alphago}. Since then, RL has made many advantages in various subdomains, one of them being Natural Language Processing (NLP). As stated in \citet{ijcai2019}, both domains influence each other. While we can gain new linguistic knowledge by getting insight into how RL agents deal with language, NLP can be used to enhance RL models. Therefore, advances in RL for NLP profits both fields. Since RL in NLP started to become more popular, various subdomains and applications emerged, such as Article summarization, Question Generation and Answering, Dialogue generation, Dialogue Systems, Machine Translation and Text generation. 

\section{Background}

\section{Purpose of NLP in RL}
While it is difficult to summarise the current state of using RL methods in NLP tasks, more can be said about the purpose of RL for NLP. RL has an easier time to deal with certain difficulties of other unsupervised or supervised methods. It often deals better with scarce or little data. It can be used to solve tasks from a different angle: \citet{ling-etal-2017-learning} enabled RL to mimic the way a human would solve a given task, for example diagnosing an illness. \citet{hu-etal-2018-playing} developed an RL agent that is optimised to select the correct questions to solve an answer-based game (Q20) without a knowledge base ---on which previous approaches relied--- and with more stability towards errors than previous models. The concept of reward in RL enables us to control the training better than is possible with supervised learning. Whether the reward is given all at the end or in intervals for each action can influence training. It is also possible to focus training on several different objectives as different rewards for different things can be combined, like \citet{mosallanezhad-etal-2019-deep} did with one reward for anonymisation and one for usefulness. Still, the success of RL models often depends on the researcher's ability to frame the problem, particularly reward functions.


\section{Trends for NLP in RL}
\textbf{Dialogues} have been among the most popular subdomains.\\\\ 
\textbf{REINFORCE} is still the most commonly used policy gradient algorithm \citep{yasui-etal-2019, zhang-2018, hu-etal-2018-playing, godin-etal-2019-learning, huang-etal-2018-neural, mao-etal-2018-end, ranzato2015sequence, wu-etal-2018-study, clark-manning-2016-deep,yogatama-2017, guu-etal-2017-language, zeng-2018}, while some others exploit \textbf{Deep Q-Learning} \citet{narasimhan-etal-2016-improving, mosallanezhad-etal-2019-deep, ling-etal-2017-learning}. Only recently a minority of RL models started to step away from REINFORCE by using different variations of the policy gradient method \citet{branavan-2009, li-etal-2018-paraphrase, li-etal-2016-deep, le-fokkens-2017-tackling} or different actor-critic methods, such as \citet{dethlefs-cuayahuitl-2011,grissom-ii-etal-2014-dont, he-etal-2016-deep-reinforcement, chen-bansal-2018-fast, goyal-2019}. 

\subsection{Deep Learning}
Deep neural architectures are still widely used with RL algorithms since they are capable of scaling to previously unsolvable problems. Applied to NLP, RL models commonly exploit RNNs in order to use the hidden vector to represent and update the environment state REFERENCES. 

For a variety of NLP-related tasks, some studies work with LSTMs. \citet{narasimhan-etal-2016-improving} tried to capture the semantics of the game states in text-based games by using an LSTM over textual data with a mean pooling layer on top. \citet{he-etal-2016-deep-reinforcement} used continuous vectors built by neural networks for text-based games. \citet{narasimhan-2018} used LSTMs in order to create a factorised state representation, concatenating an object embedding with its textual specification. For text summarisation, CNN were used at a sentence level and an LSTM or BiLSTM at document level to capture global information by \citet{chen-bansal-2018-fast}. 

\citet{yasui-etal-2019, zhang-2018, li-etal-2016-deep, ranzato2015sequence}. 

\section{Problems for NLP in RL}
Research faces a few problems regarding the application of RL in NLP. 

While most research focuses on the reward function in order to maximise the agent's expected long-term reward, \citet{madureira2020} discussed the importance of \textbf{state} in RL for NLP. While the formalisation of the state and its properties depends on the value and reward function, there is less attention directed towards the state. This shows for example in the fact that many researchers neglect describing their state signal and the reasoning of their choice, which alone makes understanding and replicating their models more difficult. With the rise of neural network models in RL for NLP, states could be more easily represented by a vector and thus more easily fed into the value or policy function. \citet{madureira2020} appealed for more linguistically interpretable and grounded state representations next to encouraging researchers to pay more attention to the choice of their state function, its explanation and respective evaluation in the paper. While this seems to be especially problematic for the state, the lack of detailed description can span across all parts of RL research.

\subsection{Data and Language}
\textbf{Anglocentrism} dominates most research fields, with NLP and more specifically RL being no exception. This is due to English continuing to be the main language in which research is written and communicated. While this is not a problem per se, it compels many researchers to also use English as their main language for conducting their experiments due to available data and funding for English research. This homogeneity is a common property of a young research field but still needs to be overcome by broadening up to other languages. This is not only important in terms of ethical diversity but also a necessary step to achieve overall application of RL across languages. \\\\
\textbf{Data} is not only limited in language but also in size and diversity. Since RL models tend to improve by larger corpora, it becomes exponentially difficult to provide datasets of sufficient quality by increase of size. Consequently, many experiments are performed on the same datasets and thus in the same language. The few studies that concern languages other than English commonly exploit machine translation, for example between English and German \citet{yasui-etal-2019}. Different studies have tried to synthesise new corpora FILL IN WHICH ONES
Some datasets have not been created specifically for training RL models but can be exploited for training them, such as the SCONE (Sequential CONtext-dependent Execution) introduced by \citet{long-2016} and applied to RL by \cite{guu-etal-2017-language}. It spans three different domains which do not include semantic annotations but only world states. Therefore it makes training ambiguous while the RL agent develops a strategy to navigate the "Alchemy", "Tangrams" and "Scene" problems.

\subsection{Knowledge Representation}
It is not only crucial for science in which language data is available but also how language is generally implemented in the other parts of an RL agent. Knowledge representation describes how real-life (or human) knowledge is encoded (=represented) in a way that a machine understands it and can use it to solve complex problems. An example of knowledge representation is the encoding of pictures or language.
\par

\textbf{Grounding} ---connecting or relating language to the real world--- is dependent on the representation of knowledge and will become one of the key features in future RL research according to \citet{narasimhan-2018}. While language needs to be encoded in a way that an RL agent can utilise it, the balance between modifying language and leaving it in its natural state such that the bridge between RL agents and the real world can be crossed. Advancing methods for grounding could make a significant different for real life applications or RL in NLP as well as give new linguistic insights.

\subsection{Evaluation measures}
The choice of evaluation measures is especially problematic in RL agents that generate language. On one hand, automating evaluation is complex and unreliable but on the other hand, human evaluation is very inefficient in time and resources. Next to the older BLEU or STS score, new approaches have been tested to automatise evaluation: \citet{yasui-etal-2019} trained a BERT-model to recognise semantically similar sentences of a translation model. Here we see that machine translated sentences can achieve human performances, but human translation is still superior when it spans more than one sentence. \\\\ 
In paraphrase generation, \citet{li-etal-2018-paraphrase} proposed training their own evaluators (together with a generator) by two different models, Reinforced by Matching-Supervised Learning (RbM-SL) and Reinforced by Matching-Inverse Reinforcement Learning (RbM-ISL). The method reminds of General Adversarial Networks (GANs). Each models perform better at a different task: RbM-SL performs better on a Quora dataset because it can make use of the additional labeled data to train the evaluator. RbM-IRL performs better on the Twitter data because its evaluator because it performs better on smaller data than its counterpart. Both models outperform 


\section{Conclusion}
As  \citet{ijcai2019} stated, "approaches combining language and RL will find applications as wide-ranging as autonomous vehicles, virtual assistants and household robots". While RL in NLP has made some advances, it is still far from being successfully exploited in Life Sciences. More careful research, especially in regard to the limitations and ethical implications, has to be conducted in order to model agents who can reliably applied to everyday life. 

\newpage
\bibliography{bib}

\end{document}