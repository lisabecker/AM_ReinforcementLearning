\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[round]{natbib}
%\usepackage[style=authoryear,sorting=ynt]{biblatex}
\bibliographystyle{plainnat}
\usepackage[hyperref]{acl2018}
\usepackage{times}
\usepackage{latexsym}
\usepackage{url}
\usepackage{array}
\usepackage{graphicx}
\usepackage[font=small,labelfont=bf]{caption}
\graphicspath{ {./img/} } 
\pagestyle{plain}

\title{{\LARGE Reinforcement Learning for Natural Language Processing}\\[1.5mm]
{\large Final paper: Literature Review Article}\\[1.5mm]} 
\author{Author: Lisa Becker (775242) } 

\begin{document}
\maketitle
\section{Abstract}


\section{Introduction}
In 2016, DeepMind put Reinforcement Learning (RL) in the spotlights by developing AlphaGo \citep{alphago}. Since then, RL has made many advantages in various subdomains, one of them being Natural Language Processing (NLP). As stated in \citet{ijcai2019}, both domains influence each other. While we can gain new linguistic knowledge by getting insight into how RL agents deal with language, NLP can be used to enhance RL models. Since RL in NLP started to become more popular, various subdomains and applications emerged, such as Article summarization, Question Generation and Answering, Dialogue generation, Dialogue Systems, Machine Translation, Text generation.

\section{Background}

\section{Current Use of NLP in RL}


\section{Trends for NLP in RL}
More Deep RL is implemented. 
\textbf{Dialogues} have been among the most popular subdomains.\\\\ 
Most RL models still utilise the the policy gradient REINFORCE algorithm or Deep Q Learning as their method.

\section{Problems for NLP in RL}
Research faces a few problems regarding the application of RL in NLP. 

While most research focuses on the reward function in order to maximize the agent's expected long-term reward, \citet{madureira2020} discussed the importance of state in RL for NLP. While the formalisation of the state and its properties depends on the value and reward function, there is less attention directed towards the state. This shows for example in the fact that many researchers neglect describing their state signal and the reasoning of their choice, which alone makes understanding and replicating their models more difficult. With the rise of neural network models in RL for NLP, states could be more easily represented by a vector and thus more easily fed into the value or policy function. \citet{madureira2020} appealed for more linguistically interpretable and grounded state representations next to encouraging researchers to pay more attention to the choice of their state function, its explanation and respective evaluation in the paper. While this seems to be especially problematic for the state, the lack of detailed description can span across all parts of RL research.

\subsection{Data and Language}
\textbf{Anglocentrism} dominates most research fields, with NLP and more specifically RL being no exception. This is due to English continuing to be the main language in which research is written and communicated. While this is not a problem per se, it compels many researchers to also use English as their main language for conducting their experiments due to available data and funding for English research. This homogeneity is a common property of a young research field but still needs to be overcome by broadening up to other languages. This is not only important in terms of ethical diversity but also a necessary step to achieve overall application of RL across languages. \\\\
\textbf{Data} is not only limited in language but also in size and diversity. Since RL models tend to improve by larger corpora, it becomes exponentially difficult to provide datasets of sufficient quality by increase of size. Consequently, many studies are performed on the same datasets.

\subsection{Evaluation measures}
The choice of evaluation measures is especially problematic in RL agents that generate language. On one hand, automating evaluation is complex and unreliable but on the other hand, human evaluation is very inefficient in time and resources. Different approaches have been tested to automatise evaluation: \citet{yasui-etal-2019} trained a BERT-model to recognise semantically similar sentences of a translation model. Here we see that machine translated sentences can achieve human performances, but human translation is still superior when it spans more than one sentence.


\section{Conclusion}
As  \citet{ijcai2019} stated, "approaches combining language and RL will find applications as wide-ranging as autonomous vehicles, virtual assistants and household robots". While RL in NLP has made some advances, it is still far from being successfully exploited in Life Sciences. More careful research, especially in regard to the limitations and ethical implications, has to be conducted in order to model agents who can reliably applied to everyday lifes. 

\newpage
\bibliography{bib}

\end{document}